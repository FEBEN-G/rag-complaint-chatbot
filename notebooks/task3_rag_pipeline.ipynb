{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Building the RAG Core Logic and Evaluation\n",
    "## Intelligent Complaint Analysis for Financial Services\n",
    "\n",
    "**Objective:** Build the retrieval and generation pipeline using the pre-built full-scale vector store, and evaluate its effectiveness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Import custom modules\n",
    "from src.rag_pipeline import RAGPipeline, EvaluationFramework\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Pre-built Embeddings and Create Vector Store\n",
    "\n",
    "First, we need to load the pre-built embeddings from the parquet file and create our ChromaDB vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if vector store already exists\n",
    "vector_store_path = Path('../vector_store')\n",
    "\n",
    "if not vector_store_path.exists() or not list(vector_store_path.glob('*')):\n",
    "    print(\"Vector store not found. Creating from pre-built embeddings...\")\n",
    "    print(\"This may take several minutes...\\n\")\n",
    "    \n",
    "    # Run the loading script\n",
    "    %run ../src/load_prebuilt_embeddings.py\nelse:\n",
    "    print(\"✅ Vector store already exists!\")\n",
    "    print(f\"Location: {vector_store_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Initialize RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the RAG pipeline\n",
    "print(\"Initializing RAG Pipeline...\\n\")\n",
    "\n",
    "rag = RAGPipeline(\n",
    "    vector_store_path='../vector_store',\n",
    "    collection_name='complaint_embeddings_full',\n",
    "    embedding_model='sentence-transformers/all-MiniLM-L6-v2'\n",
    ")\n",
    "\n",
    "print(\"\\n✅ RAG Pipeline initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Test Retrieval\n",
    "\n",
    "Let's test the retrieval component with some sample queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test query 1\n",
    "test_query = \"Why are people unhappy with Credit Cards?\"\n",
    "\n",
    "print(f\"Query: {test_query}\")\n",
    "print(\"\\nRetrieving relevant complaints...\\n\")\n",
    "\n",
    "results = rag.retrieve(query=test_query, n_results=5)\n",
    "\n",
    "print(f\"Retrieved {len(results['documents'][0])} chunks\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, (doc, metadata, distance) in enumerate(zip(\n",
    "    results['documents'][0],\n",
    "    results['metadatas'][0],\n",
    "    results['distances'][0]\n",
    "), 1):\n",
    "    print(f\"\\nResult {i}:\")\n",
    "    print(f\"  Product: {metadata.get('product', 'Unknown')}\")\n",
    "    print(f\"  Issue: {metadata.get('issue', 'Unknown')}\")\n",
    "    print(f\"  Distance: {distance:.4f}\")\n",
    "    print(f\"  Text: {doc[:200]}...\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test query 2 with product filter\n",
    "test_query_2 = \"What problems do customers have with loan payments?\"\n",
    "\n",
    "print(f\"Query: {test_query_2}\")\n",
    "print(\"Filter: Personal Loan\\n\")\n",
    "\n",
    "results_2 = rag.retrieve(\n",
    "    query=test_query_2,\n",
    "    n_results=5,\n",
    "    product_filter=\"loan\"\n",
    ")\n",
    "\n",
    "print(f\"Retrieved {len(results_2['documents'][0])} chunks\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, (doc, metadata, distance) in enumerate(zip(\n",
    "    results_2['documents'][0],\n",
    "    results_2['metadatas'][0],\n",
    "    results_2['distances'][0]\n",
    "), 1):\n",
    "    print(f\"\\nResult {i}:\")\n",
    "    print(f\"  Product: {metadata.get('product', 'Unknown')}\")\n",
    "    print(f\"  Issue: {metadata.get('issue', 'Unknown')}\")\n",
    "    print(f\"  Distance: {distance:.4f}\")\n",
    "    print(f\"  Text: {doc[:200]}...\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Test Complete RAG Pipeline\n",
    "\n",
    "Now let's test the complete pipeline including answer generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate answer for a query\n",
    "query = \"What are the main issues customers face with credit card billing?\"\n",
    "\n",
    "print(f\"Question: {query}\\n\")\n",
    "print(\"Generating answer...\\n\")\n",
    "\n",
    "response = rag.generate_answer(\n",
    "    query=query,\n",
    "    n_results=5,\n",
    "    use_llm=False  # Set to True if you have LLM configured\n",
    ")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ANSWER:\")\n",
    "print(\"=\" * 80)\n",
    "print(response['answer'])\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"Based on {response['n_sources']} source complaints\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the sources\n",
    "print(\"\\nSOURCE COMPLAINTS:\\n\")\n",
    "\n",
    "for i, source in enumerate(response['sources'][:3], 1):\n",
    "    print(f\"Source {i}:\")\n",
    "    print(f\"  Product: {source['metadata'].get('product', 'Unknown')}\")\n",
    "    print(f\"  Issue: {source['metadata'].get('issue', 'Unknown')}\")\n",
    "    print(f\"  Company: {source['metadata'].get('company', 'Unknown')}\")\n",
    "    print(f\"  Relevance Score: {1 - source['distance']:.4f}\")\n",
    "    print(f\"  Text: {source['text'][:250]}...\")\n",
    "    print(\"\\n\" + \"-\" * 80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Qualitative Evaluation\n",
    "\n",
    "Create a comprehensive evaluation with multiple test questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize evaluation framework\n",
    "evaluator = EvaluationFramework(rag)\n",
    "\n",
    "# Define test questions\n",
    "test_questions = [\n",
    "    {\n",
    "        'question': \"Why are people unhappy with Credit Cards?\",\n",
    "        'expected_themes': ['billing', 'fees', 'interest rates', 'unauthorized charges'],\n",
    "        'product_filter': None\n",
    "    },\n",
    "    {\n",
    "        'question': \"What are the most common complaints about personal loans?\",\n",
    "        'expected_themes': ['payment issues', 'interest rates', 'customer service'],\n",
    "        'product_filter': 'loan'\n",
    "    },\n",
    "    {\n",
    "        'question': \"What problems do customers report with savings accounts?\",\n",
    "        'expected_themes': ['access issues', 'fees', 'account closure'],\n",
    "        'product_filter': 'savings'\n",
    "    },\n",
    "    {\n",
    "        'question': \"What issues arise with money transfers?\",\n",
    "        'expected_themes': ['delays', 'fees', 'failed transfers', 'fraud'],\n",
    "        'product_filter': 'transfer'\n",
    "    },\n",
    "    {\n",
    "        'question': \"How do customers describe problems with unauthorized credit card charges?\",\n",
    "        'expected_themes': ['fraud', 'dispute resolution', 'refunds'],\n",
    "        'product_filter': 'credit'\n",
    "    },\n",
    "    {\n",
    "        'question': \"What are the main customer service complaints across all products?\",\n",
    "        'expected_themes': ['response time', 'unhelpful staff', 'communication'],\n",
    "        'product_filter': None\n",
    "    },\n",
    "    {\n",
    "        'question': \"What billing disputes do customers mention most frequently?\",\n",
    "        'expected_themes': ['incorrect charges', 'double billing', 'fees'],\n",
    "        'product_filter': None\n",
    "    },\n",
    "    {\n",
    "        'question': \"What are customers saying about loan application rejections?\",\n",
    "        'expected_themes': ['credit score', 'documentation', 'explanation'],\n",
    "        'product_filter': 'loan'\n",
    "    }\n",
    "]\n",
    "\n",
    "# Add questions to evaluator\n",
    "for q in test_questions:\n",
    "    evaluator.add_test_question(\n",
    "        question=q['question'],\n",
    "        expected_themes=q['expected_themes'],\n",
    "        product_filter=q['product_filter']\n",
    "    )\n",
    "\n",
    "print(f\"Added {len(test_questions)} test questions for evaluation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation\n",
    "print(\"Running evaluation...\\n\")\n",
    "results_df = evaluator.run_evaluation(n_results=5)\n",
    "\n",
    "print(\"\\n✅ Evaluation complete!\")\n",
    "print(f\"\\nResults summary:\")\n",
    "print(results_df[['question', 'n_sources', 'top_product', 'avg_distance']].to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print detailed evaluation report\n",
    "evaluator.print_evaluation_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Create Evaluation Table for Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create evaluation table for the final report\n",
    "eval_table = []\n",
    "\n",
    "for i, result in enumerate(evaluator.results, 1):\n",
    "    # Quality score (1-5) based on relevance\n",
    "    avg_distance = result['avg_distance']\n",
    "    quality_score = max(1, min(5, int((1 - avg_distance) * 5) + 1))\n",
    "    \n",
    "    # Get top 2 sources\n",
    "    sources = result['sources_preview']\n",
    "    source_summary = \"\"\n",
    "    if sources:\n",
    "        source_summary = f\"{sources[0]['metadata'].get('product', 'Unknown')} - {sources[0]['metadata'].get('issue', 'Unknown')}\"\n",
    "    \n",
    "    # Analysis\n",
    "    analysis = f\"Retrieved {result['n_sources']} relevant sources. \"\n",
    "    if avg_distance < 0.5:\n",
    "        analysis += \"High relevance - sources closely match query.\"\n",
    "    elif avg_distance < 0.7:\n",
    "        analysis += \"Good relevance - sources provide useful context.\"\n",
    "    else:\n",
    "        analysis += \"Moderate relevance - may need query refinement.\"\n",
    "    \n",
    "    eval_table.append({\n",
    "        'Question': result['question'][:60] + '...' if len(result['question']) > 60 else result['question'],\n",
    "        'Generated Answer': result['answer'][:150] + '...' if len(result['answer']) > 150 else result['answer'],\n",
    "        'Retrieved Sources': source_summary,\n",
    "        'Quality Score (1-5)': quality_score,\n",
    "        'Comments/Analysis': analysis\n",
    "    })\n",
    "\n",
    "eval_df = pd.DataFrame(eval_table)\n",
    "print(\"\\nEVALUATION TABLE FOR REPORT:\")\n",
    "print(\"=\" * 120)\n",
    "print(eval_df.to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save evaluation results\n",
    "output_path = Path('../data/processed/rag_evaluation_results.csv')\n",
    "eval_df.to_csv(output_path, index=False)\n",
    "print(f\"\\n✅ Evaluation results saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Analysis and Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze retrieval performance\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# Distance distribution\n",
    "distances = [r['avg_distance'] for r in evaluator.results]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(distances, bins=10, color='skyblue', edgecolor='black')\n",
    "plt.xlabel('Average Distance', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.title('Distribution of Retrieval Distances', fontsize=14, fontweight='bold')\n",
    "plt.axvline(np.mean(distances), color='red', linestyle='--', label=f'Mean: {np.mean(distances):.3f}')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nRetrieval Statistics:\")\n",
    "print(f\"  Mean Distance: {np.mean(distances):.4f}\")\n",
    "print(f\"  Median Distance: {np.median(distances):.4f}\")\n",
    "print(f\"  Min Distance: {np.min(distances):.4f}\")\n",
    "print(f\"  Max Distance: {np.max(distances):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Task 3 Completed! ✅\n",
    "\n",
    "**What We Built:**\n",
    "1. **Retriever**: Semantic search using ChromaDB and sentence transformers\n",
    "2. **Prompt Template**: Structured prompt for financial complaint analysis\n",
    "3. **Generator**: Answer synthesis (with LLM integration support)\n",
    "4. **Evaluation Framework**: Systematic testing with multiple queries\n",
    "\n",
    "**Key Findings:**\n",
    "- The retrieval system successfully finds relevant complaints\n",
    "- Average retrieval distance indicates good semantic matching\n",
    "- Product filtering works effectively\n",
    "- Sources are traceable and verifiable\n",
    "\n",
    "**What Works Well:**\n",
    "- Semantic search captures intent beyond keyword matching\n",
    "- Metadata filtering enables product-specific queries\n",
    "- Retrieved sources are contextually relevant\n",
    "\n",
    "**Areas for Improvement:**\n",
    "- LLM integration would provide more natural answers\n",
    "- Could experiment with different chunk sizes\n",
    "- May benefit from query expansion techniques\n",
    "\n",
    "**Next Steps:**\n",
    "- Proceed to Task 4: Build interactive UI with Gradio/Streamlit\n",
    "- Integrate this RAG pipeline into the chat interface\n",
    "- Add source citation display and streaming"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
