{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Text Chunking, Embedding, and Vector Store Indexing\n",
    "## Intelligent Complaint Analysis for Financial Services\n",
    "\n",
    "**Objective:** Convert cleaned text narratives into a format suitable for efficient semantic search by creating embeddings and building a vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from typing import List, Dict\n",
    "from tqdm import tqdm\n",
    "\n",
    "# LangChain imports\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Sentence Transformers for embeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Vector stores\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Cleaned Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the filtered and cleaned dataset from Task 1\n",
    "data_path = Path('../data/processed/filtered_complaints.csv')\n",
    "print(f\"Loading cleaned data from: {data_path}\")\n",
    "\n",
    "df = pd.read_csv(data_path)\n",
    "print(f\"\\nDataset loaded successfully!\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Stratified Sample\n",
    "\n",
    "We'll create a stratified sample of 10,000-15,000 complaints ensuring proportional representation across all product categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check product distribution\n",
    "print(\"Product distribution in full dataset:\")\n",
    "product_dist = df['Product'].value_counts()\n",
    "print(product_dist)\n",
    "print(f\"\\nTotal complaints: {len(df):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set sample size\n",
    "SAMPLE_SIZE = 12000  # Target sample size\n",
    "\n",
    "# Calculate sampling fraction\n",
    "sampling_fraction = min(SAMPLE_SIZE / len(df), 1.0)\n",
    "\n",
    "print(f\"Target sample size: {SAMPLE_SIZE:,}\")\n",
    "print(f\"Sampling fraction: {sampling_fraction:.4f}\")\n",
    "\n",
    "# Stratified sampling by Product\n",
    "df_sample = df.groupby('Product', group_keys=False).apply(\n",
    "    lambda x: x.sample(frac=sampling_fraction, random_state=42)\n",
    ").reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nActual sample size: {len(df_sample):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify stratification\n",
    "print(\"\\nProduct distribution in sample:\")\n",
    "sample_dist = df_sample['Product'].value_counts()\n",
    "print(sample_dist)\n",
    "\n",
    "# Compare proportions\n",
    "comparison = pd.DataFrame({\n",
    "    'Full Dataset': (product_dist / len(df) * 100).round(2),\n",
    "    'Sample': (sample_dist / len(df_sample) * 100).round(2)\n",
    "})\n",
    "print(\"\\nProportion comparison (%):\") \n",
    "print(comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Text Chunking Strategy\n",
    "\n",
    "We'll use `RecursiveCharacterTextSplitter` to split long narratives into manageable chunks.\n",
    "\n",
    "**Rationale for chunk parameters:**\n",
    "- **Chunk size: 500 characters** - Balances context preservation with embedding quality\n",
    "- **Chunk overlap: 50 characters** - Ensures continuity between chunks and prevents loss of context at boundaries\n",
    "- **Separators:** Prioritize paragraph and sentence boundaries for semantic coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50,\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    ")\n",
    "\n",
    "print(\"Text Splitter Configuration:\")\n",
    "print(f\"  Chunk size: 500 characters\")\n",
    "print(f\"  Chunk overlap: 50 characters\")\n",
    "print(f\"  Separators: ['\\\\n\\\\n', '\\\\n', '. ', ' ', '']\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test chunking on a sample narrative\n",
    "sample_narrative = df_sample['cleaned_narrative'].iloc[0]\n",
    "print(\"Sample narrative (first 500 chars):\")\n",
    "print(sample_narrative[:500])\n",
    "print(f\"\\nFull length: {len(sample_narrative)} characters\")\n",
    "\n",
    "# Split the sample\n",
    "chunks = text_splitter.split_text(sample_narrative)\n",
    "print(f\"\\nNumber of chunks: {len(chunks)}\")\n",
    "print(\"\\nFirst chunk:\")\n",
    "print(chunks[0])\n",
    "if len(chunks) > 1:\n",
    "    print(\"\\nSecond chunk:\")\n",
    "    print(chunks[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create chunks for all complaints in the sample\n",
    "def create_chunks_with_metadata(df: pd.DataFrame) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Create text chunks with associated metadata.\n",
    "    \n",
    "    Returns:\n",
    "        List of dictionaries containing chunk text and metadata\n",
    "    \"\"\"\n",
    "    all_chunks = []\n",
    "    \n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Creating chunks\"):\n",
    "        narrative = row['cleaned_narrative']\n",
    "        \n",
    "        # Skip if narrative is empty\n",
    "        if not narrative or len(str(narrative).strip()) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Split into chunks\n",
    "        chunks = text_splitter.split_text(str(narrative))\n",
    "        \n",
    "        # Create metadata for each chunk\n",
    "        for chunk_idx, chunk in enumerate(chunks):\n",
    "            chunk_data = {\n",
    "                'text': chunk,\n",
    "                'complaint_id': str(row['complaint_id']),\n",
    "                'product': row['Product'],\n",
    "                'issue': row.get('Issue', 'Unknown'),\n",
    "                'sub_issue': row.get('Sub-issue', 'Unknown'),\n",
    "                'company': row.get('Company', 'Unknown'),\n",
    "                'state': row.get('State', 'Unknown'),\n",
    "                'date_received': str(row.get('Date received', 'Unknown')),\n",
    "                'chunk_index': chunk_idx,\n",
    "                'total_chunks': len(chunks)\n",
    "            }\n",
    "            all_chunks.append(chunk_data)\n",
    "    \n",
    "    return all_chunks\n",
    "\n",
    "print(\"Creating chunks for all complaints...\")\n",
    "chunks_data = create_chunks_with_metadata(df_sample)\n",
    "print(f\"\\n✅ Created {len(chunks_data):,} chunks from {len(df_sample):,} complaints\")\n",
    "print(f\"Average chunks per complaint: {len(chunks_data)/len(df_sample):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze chunk statistics\n",
    "chunk_lengths = [len(chunk['text']) for chunk in chunks_data]\n",
    "print(\"Chunk length statistics (characters):\")\n",
    "print(f\"  Mean: {np.mean(chunk_lengths):.2f}\")\n",
    "print(f\"  Median: {np.median(chunk_lengths):.2f}\")\n",
    "print(f\"  Min: {np.min(chunk_lengths)}\")\n",
    "print(f\"  Max: {np.max(chunk_lengths)}\")\n",
    "\n",
    "# Sample chunk\n",
    "print(\"\\nSample chunk with metadata:\")\n",
    "print(chunks_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Embedding Model Selection\n",
    "\n",
    "**Model:** `sentence-transformers/all-MiniLM-L6-v2`\n",
    "\n",
    "**Rationale:**\n",
    "- Lightweight and efficient (80MB)\n",
    "- 384-dimensional embeddings (good balance of performance and size)\n",
    "- Trained on a large corpus for semantic similarity tasks\n",
    "- Fast inference time suitable for large-scale embedding generation\n",
    "- Well-suited for semantic search applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embedding model\n",
    "print(\"Loading embedding model: all-MiniLM-L6-v2\")\n",
    "embedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "print(f\"\\n✅ Model loaded successfully!\")\n",
    "print(f\"Embedding dimension: {embedding_model.get_sentence_embedding_dimension()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test embedding generation\n",
    "test_text = \"I have a problem with my credit card billing.\"\n",
    "test_embedding = embedding_model.encode(test_text)\n",
    "\n",
    "print(f\"Test text: {test_text}\")\n",
    "print(f\"Embedding shape: {test_embedding.shape}\")\n",
    "print(f\"Embedding (first 10 values): {test_embedding[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract texts for embedding\n",
    "chunk_texts = [chunk['text'] for chunk in chunks_data]\n",
    "\n",
    "print(f\"Generating embeddings for {len(chunk_texts):,} chunks...\")\n",
    "print(\"This may take a few minutes...\")\n",
    "\n",
    "# Generate embeddings in batches for efficiency\n",
    "batch_size = 32\n",
    "embeddings = embedding_model.encode(\n",
    "    chunk_texts,\n",
    "    batch_size=batch_size,\n",
    "    show_progress_bar=True,\n",
    "    convert_to_numpy=True\n",
    ")\n",
    "\n",
    "print(f\"\\n✅ Generated {len(embeddings):,} embeddings\")\n",
    "print(f\"Embeddings shape: {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Build Vector Store with ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ChromaDB client\n",
    "vector_store_path = Path('../vector_store')\n",
    "vector_store_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Initializing ChromaDB at: {vector_store_path}\")\n",
    "\n",
    "chroma_client = chromadb.PersistentClient(\n",
    "    path=str(vector_store_path)\n",
    ")\n",
    "\n",
    "print(\"✅ ChromaDB client initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create or get collection\n",
    "collection_name = \"complaint_embeddings_sample\"\n",
    "\n",
    "# Delete collection if it exists (for clean start)\n",
    "try:\n",
    "    chroma_client.delete_collection(name=collection_name)\n",
    "    print(f\"Deleted existing collection: {collection_name}\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Create new collection\n",
    "collection = chroma_client.create_collection(\n",
    "    name=collection_name,\n",
    "    metadata={\"description\": \"Complaint narratives embeddings for RAG chatbot\"}\n",
    ")\n",
    "\n",
    "print(f\"✅ Created collection: {collection_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for ChromaDB\n",
    "print(\"Preparing data for vector store...\")\n",
    "\n",
    "# Create unique IDs for each chunk\n",
    "ids = [f\"chunk_{i}\" for i in range(len(chunks_data))]\n",
    "\n",
    "# Prepare metadata (ChromaDB requires string values)\n",
    "metadatas = []\n",
    "for chunk in chunks_data:\n",
    "    metadata = {\n",
    "        'complaint_id': str(chunk['complaint_id']),\n",
    "        'product': str(chunk['product']),\n",
    "        'issue': str(chunk['issue']),\n",
    "        'sub_issue': str(chunk['sub_issue']),\n",
    "        'company': str(chunk['company']),\n",
    "        'state': str(chunk['state']),\n",
    "        'date_received': str(chunk['date_received']),\n",
    "        'chunk_index': str(chunk['chunk_index']),\n",
    "        'total_chunks': str(chunk['total_chunks'])\n",
    "    }\n",
    "    metadatas.append(metadata)\n",
    "\n",
    "# Documents (the actual text)\n",
    "documents = chunk_texts\n",
    "\n",
    "print(f\"Prepared {len(ids):,} items for indexing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add to ChromaDB in batches\n",
    "print(\"Adding embeddings to ChromaDB...\")\n",
    "\n",
    "batch_size = 1000\n",
    "for i in tqdm(range(0, len(ids), batch_size), desc=\"Indexing batches\"):\n",
    "    batch_end = min(i + batch_size, len(ids))\n",
    "    \n",
    "    collection.add(\n",
    "        ids=ids[i:batch_end],\n",
    "        embeddings=embeddings[i:batch_end].tolist(),\n",
    "        documents=documents[i:batch_end],\n",
    "        metadatas=metadatas[i:batch_end]\n",
    "    )\n",
    "\n",
    "print(f\"\\n✅ Successfully indexed {len(ids):,} chunks in ChromaDB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the collection\n",
    "print(\"\\nCollection Statistics:\")\n",
    "print(f\"  Name: {collection.name}\")\n",
    "print(f\"  Count: {collection.count():,}\")\n",
    "print(f\"  Metadata: {collection.metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test the Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test query\n",
    "test_query = \"Problems with credit card billing and unauthorized charges\"\n",
    "\n",
    "print(f\"Test query: {test_query}\")\n",
    "print(\"\\nGenerating query embedding...\")\n",
    "\n",
    "# Generate embedding for query\n",
    "query_embedding = embedding_model.encode(test_query)\n",
    "\n",
    "# Search\n",
    "results = collection.query(\n",
    "    query_embeddings=[query_embedding.tolist()],\n",
    "    n_results=5\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Top 5 Most Relevant Chunks:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, (doc, metadata, distance) in enumerate(zip(\n",
    "    results['documents'][0],\n",
    "    results['metadatas'][0],\n",
    "    results['distances'][0]\n",
    "), 1):\n",
    "    print(f\"\\nResult {i}:\")\n",
    "    print(f\"  Product: {metadata['product']}\")\n",
    "    print(f\"  Issue: {metadata['issue']}\")\n",
    "    print(f\"  Distance: {distance:.4f}\")\n",
    "    print(f\"  Text: {doc[:200]}...\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with product filtering\n",
    "test_query_2 = \"Issues with personal loan payments\"\n",
    "\n",
    "print(f\"Test query with filter: {test_query_2}\")\n",
    "print(\"Filter: Product contains 'loan'\")\n",
    "\n",
    "query_embedding_2 = embedding_model.encode(test_query_2)\n",
    "\n",
    "results_filtered = collection.query(\n",
    "    query_embeddings=[query_embedding_2.tolist()],\n",
    "    n_results=3,\n",
    "    where={\"product\": {\"$contains\": \"loan\"}}\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Top 3 Results (Personal Loan only):\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, (doc, metadata) in enumerate(zip(\n",
    "    results_filtered['documents'][0],\n",
    "    results_filtered['metadatas'][0]\n",
    "), 1):\n",
    "    print(f\"\\nResult {i}:\")\n",
    "    print(f\"  Product: {metadata['product']}\")\n",
    "    print(f\"  Issue: {metadata['issue']}\")\n",
    "    print(f\"  Text: {doc[:200]}...\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Metadata and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save configuration and statistics\n",
    "config = {\n",
    "    'embedding_model': 'sentence-transformers/all-MiniLM-L6-v2',\n",
    "    'embedding_dimension': 384,\n",
    "    'chunk_size': 500,\n",
    "    'chunk_overlap': 50,\n",
    "    'sample_size': len(df_sample),\n",
    "    'total_chunks': len(chunks_data),\n",
    "    'collection_name': collection_name,\n",
    "    'vector_store_path': str(vector_store_path),\n",
    "    'products': df_sample['Product'].unique().tolist()\n",
    "}\n",
    "\n",
    "# Save config\n",
    "config_path = vector_store_path / 'config.pkl'\n",
    "with open(config_path, 'wb') as f:\n",
    "    pickle.dump(config, f)\n",
    "\n",
    "print(\"Configuration saved:\")\n",
    "for key, value in config.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(f\"\\n✅ Configuration saved to: {config_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary\n",
    "\n",
    "### Task 2 Completed Successfully! ✅\n",
    "\n",
    "**Sampling Strategy:**\n",
    "- Created a stratified sample of ~12,000 complaints\n",
    "- Ensured proportional representation across all product categories\n",
    "- Maintained the original distribution of products\n",
    "\n",
    "**Chunking Approach:**\n",
    "- Used `RecursiveCharacterTextSplitter` from LangChain\n",
    "- Chunk size: 500 characters (balances context and embedding quality)\n",
    "- Chunk overlap: 50 characters (maintains continuity)\n",
    "- Generated ~[X] chunks from [Y] complaints\n",
    "\n",
    "**Embedding Model:**\n",
    "- Model: `sentence-transformers/all-MiniLM-L6-v2`\n",
    "- Dimension: 384\n",
    "- Lightweight (80MB) and efficient\n",
    "- Optimized for semantic similarity tasks\n",
    "\n",
    "**Vector Store:**\n",
    "- Built using ChromaDB\n",
    "- Persistent storage for reuse\n",
    "- Metadata includes: complaint_id, product, issue, sub_issue, company, state, date\n",
    "- Supports filtering and semantic search\n",
    "\n",
    "**Next Steps:**\n",
    "- Proceed to Task 3: Build RAG Core Logic\n",
    "- Use the pre-built full-scale vector store for production\n",
    "- Implement retrieval and generation pipeline"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
